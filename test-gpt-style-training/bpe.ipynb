{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f202affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"D:/Workspace/PYTHON/NLP/Project2/Data/sentences_cleaned.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0b0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad0aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting BPE tokenizer training...\n",
      "Training complete!\n",
      "Generated files: bpe_tokenizer.model and bpe_tokenizer.vocab\n",
      "\n",
      "--- Testing the new tokenizer ---\n",
      "\n",
      "Original: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "Encoded (pieces): ['▁ط', 'ال', 'ب', '▁علم', 'وں', '▁کا', '▁ق', 'یا', 'م', '▁و', '▁ط', 'ع', 'ام', '▁اور', '▁تح', 'قی', 'ق', '▁کا', '▁خ', 'ر', 'چ', '▁حکومت', '▁اٹھ', 'اتی', '▁ہے']\n",
      "Encoded (IDs)(len: 25): [92, 41, 737, 556, 37, 27, 68, 24, 732, 26, 92, 744, 46, 50, 345, 216, 746, 27, 48, 728, 748, 336, 379, 532, 13]\n",
      "Decoded: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "\n",
      "Original: وہ غائب تھی\n",
      "Encoded (pieces): ['▁وہ', '▁غ', 'ائ', 'ب', '▁تھی']\n",
      "Encoded (IDs)(len: 5): [64, 161, 43, 737, 155]\n",
      "Decoded: وہ غائب تھی\n",
      "\n",
      "Original: جب دل چاھے گانا\n",
      "Encoded (pieces): ['▁جب', '▁دل', '▁چ', 'ا', 'ھے', '▁گ', 'انا']\n",
      "Encoded (IDs)(len: 7): [182, 229, 45, 724, 575, 28, 282]\n",
      "Decoded: جب دل چاھے گانا\n",
      "\n",
      "Original: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "Encoded (pieces): ['▁جس', 'م', 'انی', '▁طور', '▁پر', '▁بھی', '▁م', 'تا', 'ثر', '▁ہو', 'ئیں', '▁اور', 'ک', 'یا', '▁اپ', '▁کے', '▁ع', 'لاق', 'ے', '▁میں', '▁ضر', 'ور', 'یات', '▁زندگی', '▁کی', '▁ا', 'ش', 'یا', 'ء', '▁با', 'اس', 'انی', '▁م', 'یس', 'ر', '▁ہیں', '؟']\n",
      "Encoded (IDs)(len: 37): [237, 732, 142, 460, 61, 63, 7, 47, 415, 19, 502, 50, 727, 24, 67, 23, 55, 388, 730, 22, 264, 21, 438, 515, 15, 5, 747, 24, 763, 306, 223, 142, 7, 80, 728, 39, 759]\n",
      "Decoded: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "\n",
      "Original: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "Encoded (pieces): ['▁ک', 'ور', 'و', 'نا', '▁و', 'یک', 'س', 'ین', '▁کے', '▁ب', 'او', 'جود', '▁ع', 'ال', 'می', '▁و', 'با', '▁کے', '▁مع', 'ا', 'شی', '▁نق', 'ص', 'ان', 'ات', '▁جل', 'د', '▁ختم', '▁نہیں', '▁ہوں', '▁گے']\n",
      "Encoded (IDs)(len: 31): [4, 21, 729, 51, 26, 38, 734, 83, 23, 9, 266, 359, 55, 41, 107, 26, 652, 23, 113, 724, 387, 633, 753, 17, 33, 681, 738, 580, 52, 82, 199]\n",
      "Decoded: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "\n",
      "Original: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n",
      "Encoded (pieces): ['▁وزیر', 'ا', 'ع', 'ظم', '▁کی', '▁ور', 'لڈ', '▁اک', 'ن', 'ام', 'ک', '▁ف', 'ور', 'م', '▁کے', '▁پرو', 'گر', 'ام', '▁میں', '▁کار', 'وب', 'اری', '▁ش', 'خص', 'یات', '▁سے', '▁م', 'لاق', 'ات', 'یں', '▁ش', 'ی', 'ڈ', 'ول']\n",
      "Encoded (IDs)(len: 34): [651, 724, 744, 674, 15, 421, 645, 403, 731, 46, 727, 57, 21, 732, 23, 714, 148, 46, 22, 273, 401, 103, 53, 629, 438, 35, 7, 388, 33, 8, 53, 725, 755, 126]\n",
      "Decoded: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "params = (\n",
    "    f'--input={input_file} '\n",
    "    '--model_prefix=bpe_tokenizer '\n",
    "    '--vocab_size=768 '\n",
    "    '--model_type=bpe '\n",
    "    '--character_coverage=1.0 '\n",
    "    '--max_sentence_length=100 '\n",
    "    '--pad_id=0 '\n",
    "    '--unk_id=1 '\n",
    "    '--bos_id=2 '\n",
    "    '--eos_id=3'\n",
    ")\n",
    "\n",
    "print(\"\\nStarting BPE tokenizer training...\")\n",
    "spm.SentencePieceTrainer.train(params)\n",
    "print(\"Training complete!\")\n",
    "print(\"Generated files: bpe_tokenizer.model and bpe_tokenizer.vocab\")\n",
    "\n",
    "\n",
    "# Using Trained Tokenizer ---\n",
    "print(\"\\n--- Testing the new tokenizer ---\")\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_tokenizer.model')\n",
    "\n",
    "# Example sentences to test\n",
    "test_sentences = [\n",
    "    \"طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\",\n",
    "    \"وہ غائب تھی\",\n",
    "    \"جب دل چاھے گانا\",\n",
    "   \"جسمانی طور پر بھی متاثر ہوئیں اور\"\n",
    "    \"کیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\",\n",
    "    \"کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\",#not in data\n",
    "    \"وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\",#not in data\n",
    "\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Encode: from text to a list of subword pieces\n",
    "    pieces = sp.encode_as_pieces(sentence)\n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Encoded (pieces): {pieces}\")\n",
    "\n",
    "    # Encode: from text to a list of integer IDs\n",
    "    ids = sp.encode_as_ids(sentence)\n",
    "    print(f\"Encoded (IDs)(len: {len(ids)}): {ids}\")\n",
    "\n",
    "    # Decode: from a list of IDs back to text\n",
    "    decoded_text = sp.decode_ids(ids)\n",
    "    print(f\"Decoded: {decoded_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75975e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53290684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
