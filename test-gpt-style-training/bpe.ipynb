{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f202affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"D:/Workspace/PYTHON/NLP/Project2/Data/sentences_cleaned.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0b0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad0aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting BPE tokenizer training...\n",
      "Training complete!\n",
      "Generated files: bpe_tokenizer.model and bpe_tokenizer.vocab\n",
      "\n",
      "--- Testing the new tokenizer ---\n",
      "\n",
      "Original: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "Encoded (pieces): ['▁ط', 'ال', 'ب', '▁ع', 'لم', 'وں', '▁کا', '▁ق', 'یا', 'م', '▁و', '▁ط', 'ع', 'ام', '▁اور', '▁تح', 'قی', 'ق', '▁کا', '▁خ', 'ر', 'چ', '▁حکومت', '▁اٹھ', 'ات', 'ی', '▁ہے']\n",
      "Encoded (IDs)(len: 27): [92, 41, 481, 55, 116, 37, 27, 68, 24, 476, 26, 92, 488, 46, 50, 345, 216, 490, 27, 48, 472, 492, 336, 379, 33, 469, 13]\n",
      "Decoded: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "\n",
      "Original: وہ غائب تھی\n",
      "Encoded (pieces): ['▁وہ', '▁غ', 'ائ', 'ب', '▁تھی']\n",
      "Encoded (IDs)(len: 5): [64, 161, 43, 481, 155]\n",
      "Decoded: وہ غائب تھی\n",
      "\n",
      "Original: جب دل چاھے گانا\n",
      "Encoded (pieces): ['▁جب', '▁دل', '▁چ', 'ا', 'ھ', 'ے', '▁گ', 'انا']\n",
      "Encoded (IDs)(len: 8): [182, 229, 45, 468, 483, 474, 28, 282]\n",
      "Decoded: جب دل چاھے گانا\n",
      "\n",
      "Original: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "Encoded (pieces): ['▁جس', 'م', 'انی', '▁طور', '▁پر', '▁بھی', '▁م', 'تا', 'ثر', '▁ہو', 'ئ', 'یں', '▁اور', 'ک', 'یا', '▁اپ', '▁کے', '▁ع', 'لاق', 'ے', '▁میں', '▁ضر', 'ور', 'یات', '▁ز', 'ندگی', '▁کی', '▁ا', 'ش', 'یا', 'ء', '▁با', 'اس', 'انی', '▁م', 'یس', 'ر', '▁ہیں', '؟']\n",
      "Encoded (IDs)(len: 39): [237, 476, 142, 460, 61, 63, 7, 47, 415, 19, 487, 8, 50, 471, 24, 67, 23, 55, 388, 474, 22, 264, 21, 438, 93, 439, 15, 5, 491, 24, 507, 306, 223, 142, 7, 80, 472, 39, 503]\n",
      "Decoded: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "\n",
      "Original: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "Encoded (pieces): ['▁ک', 'ور', 'و', 'نا', '▁و', 'یک', 'س', 'ین', '▁کے', '▁ب', 'او', 'جود', '▁ع', 'ال', 'می', '▁و', 'ب', 'ا', '▁کے', '▁مع', 'ا', 'شی', '▁ن', 'ق', 'ص', 'ان', 'ات', '▁ج', 'ل', 'د', '▁خ', 'تم', '▁نہیں', '▁ہوں', '▁گے']\n",
      "Encoded (IDs)(len: 35): [4, 21, 473, 51, 26, 38, 478, 83, 23, 9, 266, 359, 55, 41, 107, 26, 481, 468, 23, 113, 468, 387, 12, 490, 497, 17, 33, 16, 479, 482, 48, 374, 52, 82, 199]\n",
      "Decoded: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "\n",
      "Original: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n",
      "Encoded (pieces): ['▁و', 'ز', 'یر', 'ا', 'ع', 'ظ', 'م', '▁کی', '▁ور', 'ل', 'ڈ', '▁اک', 'ن', 'ام', 'ک', '▁ف', 'ور', 'م', '▁کے', '▁پر', 'و', 'گر', 'ام', '▁میں', '▁کار', 'وب', 'اری', '▁ش', 'خ', 'ص', 'یات', '▁سے', '▁م', 'لاق', 'ات', 'یں', '▁ش', 'ی', 'ڈ', 'ول']\n",
      "Encoded (IDs)(len: 40): [26, 494, 72, 468, 488, 502, 476, 15, 421, 479, 499, 403, 475, 46, 471, 57, 21, 476, 23, 61, 473, 148, 46, 22, 273, 401, 103, 53, 496, 497, 438, 35, 7, 388, 33, 8, 53, 469, 499, 126]\n",
      "Decoded: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "params = (\n",
    "    f'--input={input_file} '\n",
    "    '--model_prefix=bpe_tokenizer '\n",
    "    '--vocab_size=512 '\n",
    "    '--model_type=bpe '\n",
    "    '--character_coverage=1.0 '\n",
    "    '--max_sentence_length=100 '\n",
    "    '--pad_id=0 '\n",
    "    '--unk_id=1 '\n",
    "    '--bos_id=2 '\n",
    "    '--eos_id=3'\n",
    ")\n",
    "\n",
    "print(\"\\nStarting BPE tokenizer training...\")\n",
    "spm.SentencePieceTrainer.train(params)\n",
    "print(\"Training complete!\")\n",
    "print(\"Generated files: bpe_tokenizer.model and bpe_tokenizer.vocab\")\n",
    "\n",
    "\n",
    "# Using Trained Tokenizer ---\n",
    "print(\"\\n--- Testing the new tokenizer ---\")\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_tokenizer.model')\n",
    "\n",
    "# Example sentences to test\n",
    "test_sentences = [\n",
    "    \"طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\",\n",
    "    \"وہ غائب تھی\",\n",
    "    \"جب دل چاھے گانا\",\n",
    "   \"جسمانی طور پر بھی متاثر ہوئیں اور\"\n",
    "    \"کیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\",\n",
    "    \"کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\",#not in data\n",
    "    \"وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\",#not in data\n",
    "\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Encode: from text to a list of subword pieces\n",
    "    pieces = sp.encode_as_pieces(sentence)\n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Encoded (pieces): {pieces}\")\n",
    "\n",
    "    # Encode: from text to a list of integer IDs\n",
    "    ids = sp.encode_as_ids(sentence)\n",
    "    print(f\"Encoded (IDs)(len: {len(ids)}): {ids}\")\n",
    "\n",
    "    # Decode: from a list of IDs back to text\n",
    "    decoded_text = sp.decode_ids(ids)\n",
    "    print(f\"Decoded: {decoded_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1fa09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
