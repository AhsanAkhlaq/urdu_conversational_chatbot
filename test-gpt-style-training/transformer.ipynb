{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a79dce",
   "metadata": {
    "executionInfo": {
     "elapsed": 4748,
     "status": "ok",
     "timestamp": 1760940351557,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "79a79dce"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as sp\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d13d8ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12041,
     "status": "ok",
     "timestamp": 1760940363601,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "1d13d8ab",
    "outputId": "2201fcd6-e0bb-471d-e8b6-7992c71e8d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: torch in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from rouge_score) (2.0.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from nltk->rouge_score) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ncs\\anaconda3\\envs\\env_ml_2\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de9a281",
   "metadata": {
    "executionInfo": {
     "elapsed": 3191,
     "status": "ok",
     "timestamp": 1760940366791,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "6de9a281"
   },
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7144dde5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1760940366938,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "7144dde5",
    "outputId": "3a906903-1b71-4949-95f0-a6ef3a5e669a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Hu7Hqc-9NWGL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33075,
     "status": "ok",
     "timestamp": 1760940400018,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "Hu7Hqc-9NWGL",
    "outputId": "a5f7028e-b204-4330-a3ae-fe57abef8f68"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e16e9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3375,
     "status": "ok",
     "timestamp": 1760940403390,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "c8e16e9b",
    "outputId": "5ea7caa9-6d71-47ee-d607-dce125185dd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = sp.SentencePieceProcessor()\n",
    "tokenizer.Load('bpe_tokenizer.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8bd517b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1760940403451,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "e8bd517b",
    "outputId": "3fcbc4dd-43e8-4c47-ca18-31884e2a7800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 768\n",
      "PAD ID: 0\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.get_piece_size()\n",
    "PAD_ID = tokenizer.piece_to_id('<pad>')\n",
    "UNK_ID = tokenizer.piece_to_id('<unk>')\n",
    "SOS_ID = tokenizer.piece_to_id('<s>')\n",
    "EOS_ID = tokenizer.piece_to_id('</s>')\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"PAD ID: {PAD_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d95d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[520, 31, 35, 135, 90, 51, 121, 51, 52, 3]\n"
     ]
    }
   ],
   "source": [
    "s=tokenizer.Encode(\"میرا اس سے کوئی لینا دینا نہیں\",add_eos=True,add_bos=True)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vf3fB9uQNUMN",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760940403455,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "vf3fB9uQNUMN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "میرا اس سے کوئی لینا دینا نہیں\n"
     ]
    }
   ],
   "source": [
    "[520, 31, 35, 135, 90, 51, 121, 51, 52]\n",
    "print(tokenizer.decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b0d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4036f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1027,
     "status": "ok",
     "timestamp": 1760940404484,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "4d4036f3"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/content/drive/MyDrive/NLP_ASS2/TEST/sentences_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "        data = [line.strip() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae72db6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1760940404534,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "fae72db6",
    "outputId": "03f2890f-0feb-49ae-c205-b95dde9df827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset...\n",
      "Train: 8197, Val: 2050\n"
     ]
    }
   ],
   "source": [
    "# Split dataset: 80% train, 10% validation, 10% test\n",
    "print(\"Splitting dataset...\")\n",
    "random.shuffle(data)\n",
    "train_size = int(0.8 * len(data))\n",
    "\n",
    "train_groups = data[:train_size]\n",
    "val_groups = data[train_size:]\n",
    "\n",
    "print(f\"Train: {len(train_groups)}, Val: {len(val_groups)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f078f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1760940404537,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "f4f078f3"
   },
   "outputs": [],
   "source": [
    "\n",
    "class UrduChatbotDataset(Dataset):\n",
    "    \"\"\"Dataset for Urdu chatbot with teacher forcing\"\"\"\n",
    "\n",
    "    def __init__(self, sentence_groups, max_len=50):\n",
    "        self.sentence_groups = sentence_groups\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Split based on word count (2/5th for input, 3/5th for target)\n",
    "        group = self.sentence_groups[idx]\n",
    "        words = group.split()\n",
    "\n",
    "        # Calculate split point based on word count\n",
    "        total_words = len(words)\n",
    "        split_point = max(1, total_words * 2 // 5)  # At least 1 word for input\n",
    "\n",
    "        input_text = ' '.join(words[:split_point])\n",
    "        target_text = ' '.join(words[split_point:])\n",
    "\n",
    "        # Tokenize input and target\n",
    "        input_tokens = tokenizer.encode(input_text)\n",
    "        target_tokens = tokenizer.encode(target_text)\n",
    "\n",
    "        # Truncate and pad\n",
    "        input_ids = input_tokens[:self.max_len] + [PAD_ID] * (self.max_len - len(input_tokens))\n",
    "        target_ids = target_tokens[:self.max_len-1]+[EOS_ID] + [PAD_ID] * (self.max_len - len(target_tokens))\n",
    "\n",
    "        # Teacher forcing: decoder input is [START] + target[:-1], decoder target is target\n",
    "        decoder_input_ids = [SOS_ID] + target_ids[:-1]\n",
    "        decoder_target_ids = target_ids \n",
    "\n",
    "        return {\n",
    "            'encoder_input': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'decoder_input': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
    "            'decoder_target': torch.tensor(decoder_target_ids, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2446679",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1760940404548,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "e2446679"
   },
   "outputs": [],
   "source": [
    "train_dataset = UrduChatbotDataset(train_groups)\n",
    "val_dataset = UrduChatbotDataset(val_groups)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3e2b5",
   "metadata": {
    "id": "ccd3e2b5"
   },
   "source": [
    "# --------------------TRANSFORMER CODE--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding using sine and cosine\"\"\"\n",
    "    def _init_(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super()._init_()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism\"\"\"\n",
    "    def _init_(self, d_model, num_heads, dropout=0.1):\n",
    "        super()._init_()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        Q = self.split_heads(self.W_q(query))\n",
    "        K = self.split_heads(self.W_k(key))\n",
    "        V = self.split_heads(self.W_v(value))\n",
    "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    def _init_(self, d_model, d_ff, dropout=0.1):\n",
    "        super()._init_()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer Encoder Layer\"\"\"\n",
    "    def _init_(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super()._init_()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer Decoder Layer\"\"\"\n",
    "    def _init_(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super()._init_()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        self_attn_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer Encoder-Decoder Model\"\"\"\n",
    "    def _init_(self, vocab_size, d_model=256, num_heads=2, d_ff=1024,\n",
    "                 num_encoder_layers=2, num_decoder_layers=2, max_len=512,\n",
    "                 dropout=0.1, pad_idx=0):\n",
    "        super()._init_()\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.decoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        return tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.encoder_pos_encoding(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        encoder_output = x\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.decoder_pos_encoding(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.output_projection(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee15ac",
   "metadata": {
    "id": "d6ee15ac"
   },
   "source": [
    "# --------------------TRAINING-TEST CODE--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bf0bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1760940404908,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "e46bf0bf"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch with teacher forcing\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        encoder_input = batch['encoder_input'].to(device)\n",
    "        decoder_input = batch['decoder_input'].to(device)\n",
    "        decoder_target = batch['decoder_target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        target = decoder_target.reshape(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'Train loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4772c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for batch in pbar:\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            decoder_target = batch['decoder_target'].to(device)\n",
    "            \n",
    "            output = model(encoder_input, decoder_input)\n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            target = decoder_target.reshape(-1)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8399697",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760940404910,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "f8399697"
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(predictions, references):\n",
    "    \"\"\"Calculate BLEU score\"\"\"\n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(predictions, [references])\n",
    "    return score.score\n",
    "\n",
    "def calculate_rouge_l(predictions, references):\n",
    "    \"\"\"Calculate ROUGE-L score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "    scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores.append(score['rougeL'].fmeasure)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def calculate_chrf(predictions, references):\n",
    "    \"\"\"Calculate chrF score\"\"\"\n",
    "    chrf = CHRF()\n",
    "    score = chrf.corpus_score(predictions, [references])\n",
    "    return score.score\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"Calculate perplexity from loss\"\"\"\n",
    "    return math.exp(min(loss, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe0845",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1760940404924,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "5dfe0845"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterian):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating (no teacher forcing)\")\n",
    "        for batch in progress_bar:\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            decoder_target = batch['decoder_target'].to(device)\n",
    "\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            loss = criterian(output.reshape(-1, output.size(-1)), decoder_target.reshape(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'val loss': loss.item()})\n",
    "\n",
    "            # Decode predictions and references for metrics\n",
    "            pred_ids = output.argmax(dim=-1).cpu().tolist()\n",
    "            tgt_ids = decoder_target.cpu().tolist()\n",
    "            for pred, ref in zip(pred_ids, tgt_ids):\n",
    "                pred_text = tokenizer.decode(pred)\n",
    "                ref_text = tokenizer.decode(ref)\n",
    "                predictions.append(pred_text)\n",
    "                references.append(ref_text)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    bleu_score = calculate_bleu(predictions, references)\n",
    "    rouge_score = calculate_rouge_l(predictions, references)\n",
    "    chrf_score = calculate_chrf(predictions, references)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"  Val Loss:   {avg_loss:.4f}\")\n",
    "    print(f\"  BLEU:       {bleu_score:.4f}\")\n",
    "    print(f\"  ROUGE-L:    {rouge_score:.4f}\")\n",
    "    print(f\"  chrF:       {chrf_score:.4f}\")\n",
    "    print(f\"  Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'bleu': bleu_score,\n",
    "        'rouge_l': rouge_score,\n",
    "        'chrf': chrf_score,\n",
    "        'perplexity': perplexity,\n",
    "        'predictions': predictions[:10],\n",
    "        'references': references[:10]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624dec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, input_text, max_length=30, temperature=0.8, device='cpu'):\n",
    "    \"\"\"Generate text continuation using beam search\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        token_ids = tokenizer.encode(input_text)\n",
    "        encoder_ids = token_ids[:50] + [PAD_ID] * (50 - len(token_ids))\n",
    "        encoder_input = torch.tensor([encoder_ids], dtype=torch.long).to(device)\n",
    "        decoder_input = torch.tensor([[SOS_ID]], dtype=torch.long).to(device)\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            next_token_logits = output[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            if next_token.item() == EOS_ID:\n",
    "                break\n",
    "            \n",
    "            generated_tokens.append(next_token.item())\n",
    "            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        output_tokens = []\n",
    "        for idx in generated_tokens:\n",
    "            if idx not in [PAD_ID,SOS_ID,EOS_ID]:\n",
    "                token = tokenizer\n",
    "                output_tokens.append(token)\n",
    "        \n",
    "        text = ''.join(output_tokens)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57f71d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1760940404942,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "7a57f71d",
    "outputId": "76314e35-6288-426e-8316-e0f6a2ad9fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 4,080,128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Transformer(\n",
    "    vocab_size=768,\n",
    "    d_model=512,\n",
    "    num_heads=2,\n",
    "    d_ff=1024,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    max_len=50,\n",
    "    dropout=0.1,\n",
    "    pad_idx=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94b60e3",
   "metadata": {
    "executionInfo": {
     "elapsed": 5412,
     "status": "ok",
     "timestamp": 1760940410356,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "f94b60e3"
   },
   "outputs": [],
   "source": [
    "criterian=nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer =torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "#scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tJaHz3Yyx_UF",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1760940410362,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "tJaHz3Yyx_UF"
   },
   "outputs": [],
   "source": [
    "output=None\n",
    "best_bleu = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f94c9ab8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287731,
     "status": "ok",
     "timestamp": 1760942006426,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "f94c9ab8",
    "outputId": "7cd5d9b1-c9ea-4621-8263-94b761db7ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (TF=0.60): 100%|██████████| 257/257 [02:12<00:00,  1.94it/s, Train loss=4.27]\n",
      "Evaluating (no teacher forcing): 100%|██████████| 65/65 [00:10<00:00,  5.92it/s, val loss=5.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Val Loss:   5.9951\n",
      "  BLEU:       0.0170\n",
      "  ROUGE-L:    0.0000\n",
      "  chrF:       6.2281\n",
      "  Perplexity: 401.4465\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (TF=0.60): 100%|██████████| 257/257 [02:12<00:00,  1.95it/s, Train loss=4.38]\n",
      "Evaluating (no teacher forcing): 100%|██████████| 65/65 [00:11<00:00,  5.83it/s, val loss=6.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Val Loss:   6.0266\n",
      "  BLEU:       0.0108\n",
      "  ROUGE-L:    0.0000\n",
      "  chrF:       6.2077\n",
      "  Perplexity: 414.3145\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS=2\n",
    "teacher_forcing=0.6\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        #teacher_forcing=max(0,teacher_forcing-0.1)\n",
    "\n",
    "        loss =train_epoch(model,train_loader,criterian,optimizer,teacher_forcing=teacher_forcing)\n",
    "        #scheduler.step(loss)\n",
    "        output = evaluate(model, val_loader,criterian)\n",
    "\n",
    "\n",
    "        if output['bleu'] > best_bleu:\n",
    "            best_bleu = output['bleu']\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "            }, 'best_model.pt')\n",
    "            print(f\"\\n  ✓ New best model saved! BLEU: {best_bleu:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eVGuQDcW-Bv1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1760942011981,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "eVGuQDcW-Bv1",
    "outputId": "00f66820-c566-42da-b4d4-909cf979ff61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\n",
      "  Example 1:\n",
      "    Prediction: کی حقثیت سے ہے؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟ ہیں؟\n",
      "    Reference:  ڈرامے کا نام نہیں ہے\n",
      "\n",
      "  Example 2:\n",
      "    Prediction: سے ہے؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟\n",
      "    Reference:  الوداع کیا\n",
      "\n",
      "  Example 3:\n",
      "    Prediction: کےدیدی کے لیےدیدی کے لیےدیدی کے لیےدیدی کے لیےدیدی کے لیےدیدی کے لیےتوں کی ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے\n",
      "    Reference:  ہمارے پاکستانی سیاستدا نوں کی ایکسپورٹ کردہ ہوگی\n",
      "\n",
      "  Example 4:\n",
      "    Prediction: سے زیادہ تو نہیں ہےرہ نہیں ہےا ہےا ہےتی ہےے نہیں ہےتی ہےتی ہےے ہیںا ہےے ہیںے ہیںے نہیں ہےے ہیںے ہیںے ہیںے ہیںے ہیںے نہیں ہےے ہیںے ہیں\n",
      "    Reference:  الطائی میں منقسم کیا گیا ہے\n",
      "\n",
      "  Example 5:\n",
      "    Prediction: کے لیے اس کا حصہ ہے؟ کے لیےابلفت کے لیےابل کے لیےابل کے لیےابل کے لیےابل کے لیےابل کے لیےابل کے لیےابل کے لیےابل کے لیےاب\n",
      "    Reference:  تک جاری رہ سکتا ہی\n",
      "\n",
      "  Example 6:\n",
      "    Prediction: کی تششششششششششششششششششریف کا ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے\n",
      "    Reference:  ہے عشرے درکار ہوں گے\n",
      "\n",
      "  Example 7:\n",
      "    Prediction: بند کر دیا ہے کہ وہ اسکٹکٹنگ کی تاریخلاف ملازم کر دیا ہےایا ہےایت لیا گیا ہے کر دیا ہےایا ہے کر دیا ہےایا ہے کر دیا ہےایا\n",
      "    Reference:  اپنے کارخانے میں بنایا تھا اور اب اس کی فروخت کر دی گئی ہے\n",
      "\n",
      "  Example 8:\n",
      "    Prediction: کا فائنل کا ایمتبہ ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے\n",
      "    Reference:  کے وزیر خزانہ نہیں رہ چکے؟\n",
      "\n",
      "  Example 9:\n",
      "    Prediction: لیےابوعی کا ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے ہے\n",
      "    Reference:  لئے تیار نہ تھی\n",
      "\n",
      "  Example 10:\n",
      "    Prediction: سے زیادہ کوئی نہیں ہوتا ہوں؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟\n",
      "    Reference:  سے بے عزتی کروانے نہیں ایا\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\")\n",
    "for i in range(10):\n",
    "\n",
    "    print(f\"\\n  Example {i+1}:\")\n",
    "    print(f\"    Prediction: {output['predictions'][i]}\")\n",
    "    print(f\"    Reference:  {output['references'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006da05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\"یہ ایک\", \"پاکستان میں\", \"اچھا\"]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    generated = generate_text(model, tokenizer, input_text, max_length=20, device=device)\n",
    "    print(f\"Input:  {input_text}\")\n",
    "    print(f\"Output: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LgAK-wBe4FEL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2305,
     "status": "ok",
     "timestamp": 1760937116766,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "LgAK-wBe4FEL",
    "outputId": "c9099a02-1c7d-4b43-dd4d-2949cdeb3fd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (no teacher forcing): 100%|██████████| 49/49 [00:01<00:00, 27.26it/s, val loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Val Loss:   3.5039\n",
      "  BLEU:       1.2726\n",
      "  ROUGE-L:    0.0000\n",
      "  chrF:       3.2106\n",
      "  Perplexity: 33.2460\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt')['model_state_dict'])\n",
    "output = evaluate(model, val_loader,criterian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "x5UM211COFdN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12924,
     "status": "ok",
     "timestamp": 1760941341046,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "x5UM211COFdN",
    "outputId": "00d35a11-d164-4060-eb2e-d04d58afc3ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (no teacher forcing): 100%|██████████| 65/65 [00:12<00:00,  5.26it/s, val loss=6.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Val Loss:   7.2425\n",
      "  BLEU:       0.0141\n",
      "  ROUGE-L:    0.0000\n",
      "  chrF:       7.1527\n",
      "  Perplexity: 1397.5314\n"
     ]
    }
   ],
   "source": [
    "output = evaluate(model, val_loader,criterian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "jjwgfGtFVi8s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1760941341112,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "jjwgfGtFVi8s",
    "outputId": "08c206b0-e250-4661-9161-fa944f263424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\n",
      "  Example 1:\n",
      "    Prediction: کی باتیں نہیں ہے؟تا ہے؟ے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گاڑیوں کی تشریف کی تشریف کی تشریف کی\n",
      "    Reference:  ڈرامے کا نام نہیں ہے\n",
      "\n",
      "  Example 2:\n",
      "    Prediction: کہیں کہنا چاہیے ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے\n",
      "    Reference:  الوداع کیا\n",
      "\n",
      "  Example 3:\n",
      "    Prediction: ایک بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار بار\n",
      "    Reference:  ہمارے پاکستانی سیاستدا نوں کی ایکسپورٹ کردہ ہوگی\n",
      "\n",
      "  Example 4:\n",
      "    Prediction: سے زیادہ توقعے نہیں ہے؟تا ہے؟ے گا؟ا ہے؟ا ہے؟ا ہے؟ ہے کہ یہ سفر ہے کہ یہ سفر ہے کہ یہ سفر ہے کہ یہ سفر ہے کہ یہ سفر ہے کہ\n",
      "    Reference:  الطائی میں منقسم کیا گیا ہے\n",
      "\n",
      "  Example 5:\n",
      "    Prediction: کے ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ ساتھ\n",
      "    Reference:  تک جاری رہ سکتا ہی\n",
      "\n",
      "  Example 6:\n",
      "    Prediction: کا ایک نکالکلنا ہے؟ ہے؟ ہے؟ ہے کہاں سے مار دیا ہے؟ے گی؟ے گی؟ے گی؟ے گی؟ے گی؟ے گی وہاں ہے؟ے گی وہاں ہے؟ے\n",
      "    Reference:  ہے عشرے درکار ہوں گے\n",
      "\n",
      "  Example 7:\n",
      "    Prediction: میدان میں اضافہ کر دیا ہے اور سفر کر دیا ہے اور اسٹس کو مار دیا ہے اور سفر کر دیا ہے اور اسٹس کو مار دیا ہے اور اسٹریلیا ہے کہیں گے\n",
      "    Reference:  اپنے کارخانے میں بنایا تھا اور اب اس کی فروخت کر دی گئی ہے\n",
      "\n",
      "  Example 8:\n",
      "    Prediction: کا اظہ ہے؟یشہ ہے اور بات ہے؟ی ہے؟ی ہے؟ی ہے اور باتیں گے؟ی ہے؟ی ہے؟ی ہے؟ی ہے؟تی ہے؟ی ہے؟ی ہے؟ی ہے\n",
      "    Reference:  کے وزیر خزانہ نہیں رہ چکے؟\n",
      "\n",
      "  Example 9:\n",
      "    Prediction: بعد بند کر دیا گیا ہے اور پھایا جائے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گا؟ے گاڑیوں میں ائے ہیں اور انڈیا جائے گا؟ے گا\n",
      "    Reference:  لئے تیار نہ تھی\n",
      "\n",
      "  Example 10:\n",
      "    Prediction: یہ بات ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے؟ ہے کہاں سے زیادہ تو نہیں ہے؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟؟\n",
      "    Reference:  سے بے عزتی کروانے نہیں ایا\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\")\n",
    "for i in range(10):\n",
    "    print(f\"\\n  Example {i+1}:\")\n",
    "    print(f\"    Prediction: {output['predictions'][i]}\")\n",
    "    print(f\"    Reference:  {output['references'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "pqidcuHngor2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1650,
     "status": "ok",
     "timestamp": 1760942262399,
     "user": {
      "displayName": "Muhammad Ahsan",
      "userId": "03077261232043266279"
     },
     "user_tz": -300
    },
    "id": "pqidcuHngor2",
    "outputId": "6c581796-6d00-4089-8c38-acd1ec0bc2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'span_15_3.pth'\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/content/drive/MyDrive/NLP_ASS2/span_15_3.pth\"\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, save_path)\n",
    "print(\"Model saved as 'span_15_3.pth'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gbb56WYcglmW",
   "metadata": {
    "id": "gbb56WYcglmW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
