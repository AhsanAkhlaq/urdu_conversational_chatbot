{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f202affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"D:/Workspace/PYTHON/NLP/Project2/Data/sentences_cleaned.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc0b0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93b7bacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the new tokenizer ---\n",
      "\n",
      "Original: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "Encoded (pieces): ['▁طالب', '▁علم', 'وں', '▁کا', '▁قیام', '▁و', '▁', 'ط', 'ع', 'ام', '▁اور', '▁تحقیق', '▁کا', '▁خ', 'ر', 'چ', '▁حکومت', '▁اٹھا', 'تی', '▁ہے']\n",
      "Encoded (IDs(len: 20)): [991, 355, 23, 14, 1009, 51, 27, 80, 55, 88, 26, 671, 14, 151, 13, 91, 200, 567, 92, 7]\n",
      "Decoded: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "\n",
      "Original: وہ غائب تھی\n",
      "Encoded (pieces): ['▁وہ', '▁غ', 'ائ', 'ب', '▁تھی']\n",
      "Encoded (IDs(len: 5)): [47, 188, 98, 28, 125]\n",
      "Decoded: وہ غائب تھی\n",
      "\n",
      "Original: جب دل چاھے گانا\n",
      "Encoded (pieces): ['▁جب', '▁دل', '▁چ', 'ا', 'ھ', 'ے', '▁گا', 'نا']\n",
      "Encoded (IDs(len: 8)): [129, 192, 108, 6, 97, 9, 83, 59]\n",
      "Decoded: جب دل چاھے گانا\n",
      "\n",
      "Original: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "Encoded (pieces): ['▁جس', 'مان', 'ی', '▁طور', '▁پر', '▁بھی', '▁م', 'تا', 'ث', 'ر', '▁ہو', 'ئیں', '▁اور', 'کی', 'ا', '▁اپ', '▁کے', '▁علاق', 'ے', '▁میں', '▁ضروری', 'ات', '▁زندگی', '▁کی', '▁ا', 'ش', 'ی', 'ا', 'ء', '▁با', 'اس', 'انی', '▁می', 'سر', '▁ہیں', '؟']\n",
      "Encoded (IDs(len: 36)): [168, 347, 5, 322, 33, 41, 36, 74, 186, 13, 40, 560, 26, 227, 6, 96, 11, 542, 9, 12, 595, 50, 385, 10, 30, 44, 5, 6, 499, 171, 271, 255, 273, 374, 21, 65]\n",
      "Decoded: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "\n",
      "Original: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "Encoded (pieces): ['▁کورونا', '▁و', 'ی', 'کس', 'ین', '▁کے', '▁باوجود', '▁عالم', 'ی', '▁و', 'ب', 'ا', '▁کے', '▁معاش', 'ی', '▁نقصان', 'ات', '▁جلد', '▁ختم', '▁نہیں', '▁ہوں', '▁گے']\n",
      "Encoded (IDs(len: 22)): [871, 51, 5, 442, 75, 11, 831, 454, 5, 51, 28, 6, 11, 881, 5, 936, 50, 696, 434, 34, 54, 163]\n",
      "Decoded: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "\n",
      "Original: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n",
      "Encoded (pieces): ['▁وزیراعظم', '▁کی', '▁ورلڈ', '▁اک', 'نا', 'م', 'ک', '▁ف', 'ور', 'م', '▁کے', '▁پروگرام', '▁میں', '▁کاروبار', 'ی', '▁شخص', 'ی', 'ات', '▁سے', '▁ملاقات', 'یں', '▁ش', 'ی', 'ڈ', 'ول']\n",
      "Encoded (IDs(len: 25)): [924, 10, 586, 460, 59, 18, 29, 100, 71, 18, 11, 863, 12, 752, 5, 602, 5, 50, 16, 755, 60, 147, 5, 63, 119]\n",
      "Decoded: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n",
      "\n",
      "Sentinel Token\n",
      "Token: '<mask>', ID: 4\n"
     ]
    }
   ],
   "source": [
    "params = (\n",
    "    f'--input={input_file} '\n",
    "    '--model_prefix=unigram_tokenizer ' \n",
    "    '--vocab_size=1024 '\n",
    "    '--model_type=unigram ' \n",
    "    '--character_coverage=1.0 '\n",
    "    '--max_sentence_length=4096 '\n",
    "    '--pad_id=0 '\n",
    "    '--unk_id=1 '\n",
    "    '--bos_id=2 '\n",
    "    '--eos_id=3 '\n",
    "    # Add our sentinel tokens as user-defined symbols.\n",
    "    f'--user_defined_symbols={\"<mask>,\"}'\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(params)\n",
    "\n",
    "#Using Trained Tokenizer\n",
    "print(\"\\n--- Testing the new tokenizer ---\")\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('unigram_tokenizer.model')\n",
    "\n",
    "# Example sentences to test\n",
    "test_sentences = [\n",
    "    \"طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\",\n",
    "    \"وہ غائب تھی\",\n",
    "    \"جب دل چاھے گانا\",\n",
    "   \"جسمانی طور پر بھی متاثر ہوئیں اور\"\n",
    "    \"کیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\",\n",
    "    \"کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\",#not in data\n",
    "    \"وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\",#not in data\n",
    "\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Encode: from text to a list of subword pieces\n",
    "    pieces = sp.encode_as_pieces(sentence)\n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Encoded (pieces): {pieces}\")\n",
    "\n",
    "    # Encode: from text to a list of integer IDs\n",
    "    ids = sp.encode_as_ids(sentence)\n",
    "    print(f\"Encoded (IDs(len: {len(ids)})): {ids}\")\n",
    "\n",
    "    # Decode: from a list of IDs back to text\n",
    "    decoded_text = sp.decode_ids(ids)\n",
    "    print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "# Sentinel Token\n",
    "print(\"\\nSentinel Token\")\n",
    "token_id = sp.piece_to_id('<mask>')\n",
    "print(f\"Token: '<mask>', ID: {token_id}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75975e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
