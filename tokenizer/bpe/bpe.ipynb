{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f202affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"D:/Workspace/PYTHON/NLP/Project2/Data/sentences_cleaned.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0b0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad0aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting BPE tokenizer training...\n",
      "Training complete!\n",
      "Generated files: bpe_tokenizer.model and bpe_tokenizer.vocab\n",
      "\n",
      "--- Testing the new tokenizer ---\n",
      "\n",
      "Original: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "Encoded (pieces): ['▁ط', 'ال', 'ب', '▁ع', 'لم', 'وں', '▁کا', '▁ق', 'یا', 'م', '▁و', '▁ط', 'ع', 'ام', '▁اور', '▁تح', 'قی', 'ق', '▁کا', '▁خ', 'ر', 'چ', '▁حکومت', '▁اٹھ', 'ات', 'ی', '▁ہے']\n",
      "Encoded (IDs)(len: 27): [95, 37, 480, 57, 130, 34, 26, 64, 25, 476, 27, 95, 489, 46, 50, 318, 203, 490, 26, 48, 471, 495, 301, 440, 36, 469, 16]\n",
      "Decoded: طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\n",
      "\n",
      "Original: وہ غائب تھی\n",
      "Encoded (pieces): ['▁وہ', '▁غ', 'ائ', 'ب', '▁تھی']\n",
      "Encoded (IDs)(len: 5): [79, 160, 39, 480, 184]\n",
      "Decoded: وہ غائب تھی\n",
      "\n",
      "Original: جب دل چاھے گانا\n",
      "Encoded (pieces): ['▁جب', '▁دل', '▁چ', 'ا', 'ھے', '▁گ', 'انا']\n",
      "Encoded (IDs)(len: 7): [189, 253, 47, 468, 403, 30, 321]\n",
      "Decoded: جب دل چاھے گانا\n",
      "\n",
      "Original: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "Encoded (pieces): ['▁جس', 'م', 'انی', '▁طور', '▁پر', '▁بھی', '▁م', 'تا', 'ثر', '▁ہو', 'ئ', 'یں', '▁اور', 'ک', 'یا', '▁اپ', '▁کے', '▁ع', 'لاق', 'ے', '▁میں', '▁ضر', 'ور', 'یات', '▁ز', 'ندگی', '▁کی', '▁ا', 'ش', 'یا', 'ء', '▁با', 'اس', 'انی', '▁م', 'یس', 'ر', '▁ہیں', '؟']\n",
      "Encoded (IDs)(len: 39): [244, 476, 142, 451, 58, 69, 8, 52, 429, 24, 487, 9, 50, 470, 25, 71, 22, 57, 344, 475, 23, 271, 21, 459, 91, 455, 15, 6, 491, 25, 507, 313, 224, 142, 8, 81, 471, 43, 504]\n",
      "Decoded: جسمانی طور پر بھی متاثر ہوئیں اورکیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\n",
      "\n",
      "Original: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "Encoded (pieces): ['▁ک', 'ور', 'و', 'نا', '▁و', 'یک', 'س', 'ین', '▁کے', '▁ب', 'او', 'جود', '▁ع', 'ال', 'می', '▁و', 'ب', 'ا', '▁کے', '▁مع', 'ا', 'شی', '▁ن', 'ق', 'ص', 'ان', 'ات', '▁ج', 'ل', 'د', '▁خ', 'تم', '▁نہیں', '▁ہوں', '▁گے']\n",
      "Encoded (IDs)(len: 35): [5, 21, 473, 55, 27, 38, 478, 62, 22, 10, 263, 312, 57, 37, 93, 27, 480, 468, 22, 127, 468, 393, 14, 490, 497, 17, 36, 18, 479, 482, 48, 384, 60, 90, 234]\n",
      "Decoded: کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\n",
      "\n",
      "Original: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n",
      "Encoded (pieces): ['▁وز', 'یر', 'ا', 'ع', 'ظ', 'م', '▁کی', '▁ور', 'ل', 'ڈ', '▁اک', 'ن', 'ام', 'ک', '▁ف', 'ور', 'م', '▁کے', '▁پر', 'و', 'گر', 'ام', '▁میں', '▁کار', 'وب', 'اری', '▁ش', 'خ', 'ص', 'یات', '▁سے', '▁م', 'لاق', 'ات', 'یں', '▁ش', 'ی', 'ڈ', 'ول']\n",
      "Encoded (IDs)(len: 39): [388, 74, 468, 489, 502, 476, 15, 363, 479, 498, 347, 474, 46, 470, 53, 21, 476, 22, 58, 473, 134, 46, 23, 241, 335, 97, 51, 496, 497, 459, 35, 8, 344, 36, 9, 51, 469, 498, 115]\n",
      "Decoded: وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\n",
      "\n",
      "Sentinel Token\n",
      "Token: '<mask>', ID: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "params = (\n",
    "    f'--input={input_file} '\n",
    "    '--model_prefix=bpe_tokenizer '\n",
    "    '--vocab_size=512 '\n",
    "    '--model_type=bpe '\n",
    "    '--character_coverage=1.0 '\n",
    "    '--max_sentence_length=4096 '\n",
    "    '--pad_id=0 '\n",
    "    '--unk_id=1 '\n",
    "    '--bos_id=2 '\n",
    "    '--eos_id=3 '\n",
    "    f'--user_defined_symbols={\"<mask>,\"}'\n",
    ")\n",
    "\n",
    "print(\"\\nStarting BPE tokenizer training...\")\n",
    "spm.SentencePieceTrainer.train(params)\n",
    "print(\"Training complete!\")\n",
    "print(\"Generated files: bpe_tokenizer.model and bpe_tokenizer.vocab\")\n",
    "\n",
    "\n",
    "# Using Trained Tokenizer ---\n",
    "print(\"\\n--- Testing the new tokenizer ---\")\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_tokenizer.model')\n",
    "\n",
    "# Example sentences to test\n",
    "test_sentences = [\n",
    "    \"طالب علموں کا قیام و طعام اور تحقیق کا خرچ حکومت اٹھاتی ہے\",\n",
    "    \"وہ غائب تھی\",\n",
    "    \"جب دل چاھے گانا\",\n",
    "   \"جسمانی طور پر بھی متاثر ہوئیں اور\"\n",
    "    \"کیا اپ کے علاقے میں ضروریات زندگی کی اشیاء بااسانی میسر ہیں؟\",\n",
    "    \"کورونا ویکسین کے باوجود عالمی وبا کے معاشی نقصانات جلد ختم نہیں ہوں گے\",#not in data\n",
    "    \"وزیراعظم کی ورلڈ اکنامک فورم کے پروگرام میں کاروباری شخصیات سے ملاقاتیں شیڈول\",#not in data\n",
    "\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Encode: from text to a list of subword pieces\n",
    "    pieces = sp.encode_as_pieces(sentence)\n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Encoded (pieces): {pieces}\")\n",
    "\n",
    "    # Encode: from text to a list of integer IDs\n",
    "    ids = sp.encode_as_ids(sentence)\n",
    "    print(f\"Encoded (IDs)(len: {len(ids)}): {ids}\")\n",
    "\n",
    "    # Decode: from a list of IDs back to text\n",
    "    decoded_text = sp.decode_ids(ids)\n",
    "    print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "\n",
    "# Sentinel Token\n",
    "print(\"\\nSentinel Token\")\n",
    "token_id = sp.piece_to_id('<mask>')\n",
    "print(f\"Token: '<mask>', ID: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75975e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53290684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
